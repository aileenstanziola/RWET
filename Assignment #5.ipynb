{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"pg143.txt\").read()\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_model(model, n, seq):\n",
    "    # make a copy of seq and append None to the end\n",
    "    seq = list(seq[:]) + [None]\n",
    "    for i in range(len(seq)-n):\n",
    "        # tuple because we're using it as a dict key!\n",
    "        gram = tuple(seq[i:i+n])\n",
    "        next_item = seq[i+n]            \n",
    "        if gram not in model:\n",
    "            model[gram] = []\n",
    "        model[gram].append(next_item)\n",
    "\n",
    "def markov_model(n, seq):\n",
    "    model = {}\n",
    "    add_to_model(model, n, seq)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg143_model = markov_model(3, open(\"pg143.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('I', 'suspect', 'we'): ['could'],\n",
       " ('suspect', 'we', 'could'): ['have'],\n",
       " ('we', 'could', 'have'): ['done'],\n",
       " ('could', 'have', 'done'): ['the'],\n",
       " ('have', 'done', 'the'): ['whole'],\n",
       " ('done', 'the', 'whole'): ['thing'],\n",
       " ('the', 'whole', 'thing'): ['on'],\n",
       " ('whole', 'thing', 'on'): ['acid'],\n",
       " ('thing', 'on', 'acid'): ['…'],\n",
       " ('on', 'acid', '…'): ['except'],\n",
       " ('acid', '…', 'except'): ['for'],\n",
       " ('…', 'except', 'for'): ['some'],\n",
       " ('except', 'for', 'some'): ['of'],\n",
       " ('for', 'some', 'of'): ['the'],\n",
       " ('some', 'of', 'the'): ['people;'],\n",
       " ('of', 'the', 'people;'): ['there'],\n",
       " ('the', 'people;', 'there'): ['were'],\n",
       " ('people;', 'there', 'were'): ['faces'],\n",
       " ('there', 'were', 'faces'): ['and'],\n",
       " ('were', 'faces', 'and'): ['bodies'],\n",
       " ('faces', 'and', 'bodies'): ['in'],\n",
       " ('and', 'bodies', 'in'): ['that'],\n",
       " ('bodies', 'in', 'that'): ['group'],\n",
       " ('in', 'that', 'group'): ['who'],\n",
       " ('that', 'group', 'who'): ['would'],\n",
       " ('group', 'who', 'would'): ['have'],\n",
       " ('who', 'would', 'have'): ['been'],\n",
       " ('would', 'have', 'been'): ['absolutely'],\n",
       " ('have', 'been', 'absolutely'): ['unendurable'],\n",
       " ('been', 'absolutely', 'unendurable'): ['on'],\n",
       " ('absolutely', 'unendurable', 'on'): ['acid.'],\n",
       " ('unendurable', 'on', 'acid.'): ['The'],\n",
       " ('on', 'acid.', 'The'): ['sight'],\n",
       " ('acid.', 'The', 'sight'): ['of'],\n",
       " ('The', 'sight', 'of'): ['a'],\n",
       " ('sight', 'of', 'a'): ['355-pound'],\n",
       " ('of', 'a', '355-pound'): ['police'],\n",
       " ('a', '355-pound', 'police'): ['chief'],\n",
       " ('355-pound', 'police', 'chief'): ['from'],\n",
       " ('police', 'chief', 'from'): ['Waco,'],\n",
       " ('chief', 'from', 'Waco,'): ['Texas,'],\n",
       " ('from', 'Waco,', 'Texas,'): ['necking'],\n",
       " ('Waco,', 'Texas,', 'necking'): ['openly'],\n",
       " ('Texas,', 'necking', 'openly'): ['with'],\n",
       " ('necking', 'openly', 'with'): ['his'],\n",
       " ('openly', 'with', 'his'): ['290-pound'],\n",
       " ('with', 'his', '290-pound'): ['wife'],\n",
       " ('his', '290-pound', 'wife'): ['(or'],\n",
       " ('290-pound', 'wife', '(or'): ['whatever'],\n",
       " ('wife', '(or', 'whatever'): ['woman'],\n",
       " ('(or', 'whatever', 'woman'): ['he'],\n",
       " ('whatever', 'woman', 'he'): ['had'],\n",
       " ('woman', 'he', 'had'): ['with'],\n",
       " ('he', 'had', 'with'): ['him)'],\n",
       " ('had', 'with', 'him)'): ['when'],\n",
       " ('with', 'him)', 'when'): ['the'],\n",
       " ('him)', 'when', 'the'): ['lights'],\n",
       " ('when', 'the', 'lights'): ['were'],\n",
       " ('the', 'lights', 'were'): ['turned'],\n",
       " ('lights', 'were', 'turned'): ['off'],\n",
       " ('were', 'turned', 'off'): ['for'],\n",
       " ('turned', 'off', 'for'): ['a'],\n",
       " ('off', 'for', 'a'): ['Dope'],\n",
       " ('for', 'a', 'Dope'): ['Film'],\n",
       " ('a', 'Dope', 'Film'): ['was'],\n",
       " ('Dope', 'Film', 'was'): ['just'],\n",
       " ('Film', 'was', 'just'): ['barely'],\n",
       " ('was', 'just', 'barely'): ['tolerable'],\n",
       " ('just', 'barely', 'tolerable'): ['on'],\n",
       " ('barely', 'tolerable', 'on'): ['mescaline'],\n",
       " ('tolerable', 'on', 'mescaline'): ['–'],\n",
       " ('on', 'mescaline', '–'): ['which'],\n",
       " ('mescaline', '–', 'which'): ['is'],\n",
       " ('–', 'which', 'is'): ['mainly'],\n",
       " ('which', 'is', 'mainly'): ['a'],\n",
       " ('is', 'mainly', 'a'): ['sensual/surface'],\n",
       " ('mainly', 'a', 'sensual/surface'): ['drug'],\n",
       " ('a', 'sensual/surface', 'drug'): ['that'],\n",
       " ('sensual/surface', 'drug', 'that'): ['exaggerates'],\n",
       " ('drug', 'that', 'exaggerates'): ['reality,'],\n",
       " ('that', 'exaggerates', 'reality,'): ['instead'],\n",
       " ('exaggerates', 'reality,', 'instead'): ['of'],\n",
       " ('reality,', 'instead', 'of'): ['altering'],\n",
       " ('instead', 'of', 'altering'): ['it'],\n",
       " ('of', 'altering', 'it'): ['–'],\n",
       " ('altering', 'it', '–'): ['but'],\n",
       " ('it', '–', 'but'): ['with'],\n",
       " ('–', 'but', 'with'): ['a'],\n",
       " ('but', 'with', 'a'): ['head'],\n",
       " ('with', 'a', 'head'): ['full'],\n",
       " ('a', 'head', 'full'): ['of'],\n",
       " ('head', 'full', 'of'): ['acid,'],\n",
       " ('full', 'of', 'acid,'): ['the'],\n",
       " ('of', 'acid,', 'the'): ['sight'],\n",
       " ('acid,', 'the', 'sight'): ['of'],\n",
       " ('the', 'sight', 'of'): ['two'],\n",
       " ('sight', 'of', 'two'): ['fantastically'],\n",
       " ('of', 'two', 'fantastically'): ['obese'],\n",
       " ('two', 'fantastically', 'obese'): ['human'],\n",
       " ('fantastically', 'obese', 'human'): ['beings'],\n",
       " ('obese', 'human', 'beings'): ['far'],\n",
       " ('human', 'beings', 'far'): ['gone'],\n",
       " ('beings', 'far', 'gone'): ['in'],\n",
       " ('far', 'gone', 'in'): ['a'],\n",
       " ('gone', 'in', 'a'): ['public'],\n",
       " ('in', 'a', 'public'): ['grope'],\n",
       " ('a', 'public', 'grope'): ['while'],\n",
       " ('public', 'grope', 'while'): ['a'],\n",
       " ('grope', 'while', 'a'): ['thousand'],\n",
       " ('while', 'a', 'thousand'): ['cops'],\n",
       " ('a', 'thousand', 'cops'): ['all'],\n",
       " ('thousand', 'cops', 'all'): ['around'],\n",
       " ('cops', 'all', 'around'): ['them'],\n",
       " ('all', 'around', 'them'): ['watched'],\n",
       " ('around', 'them', 'watched'): ['a'],\n",
       " ('them', 'watched', 'a'): ['movie'],\n",
       " ('watched', 'a', 'movie'): ['about'],\n",
       " ('a', 'movie', 'about'): ['the'],\n",
       " ('movie', 'about', 'the'): ['“dangers'],\n",
       " ('about', 'the', '“dangers'): ['of'],\n",
       " ('the', '“dangers', 'of'): ['marijuana”'],\n",
       " ('“dangers', 'of', 'marijuana”'): ['would'],\n",
       " ('of', 'marijuana”', 'would'): ['not'],\n",
       " ('marijuana”', 'would', 'not'): ['be'],\n",
       " ('would', 'not', 'be'): ['emotionally'],\n",
       " ('not', 'be', 'emotionally'): ['acceptable.'],\n",
       " ('be', 'emotionally', 'acceptable.'): ['The'],\n",
       " ('emotionally', 'acceptable.', 'The'): ['brain'],\n",
       " ('acceptable.', 'The', 'brain'): ['would'],\n",
       " ('The', 'brain', 'would'): ['reject'],\n",
       " ('brain', 'would', 'reject'): ['it:'],\n",
       " ('would', 'reject', 'it:'): ['The'],\n",
       " ('reject', 'it:', 'The'): ['medulla'],\n",
       " ('it:', 'The', 'medulla'): ['would'],\n",
       " ('The', 'medulla', 'would'): ['attempt'],\n",
       " ('medulla', 'would', 'attempt'): ['to'],\n",
       " ('would', 'attempt', 'to'): ['close'],\n",
       " ('attempt', 'to', 'close'): ['itself'],\n",
       " ('to', 'close', 'itself'): ['off'],\n",
       " ('close', 'itself', 'off'): ['from'],\n",
       " ('itself', 'off', 'from'): ['the'],\n",
       " ('off', 'from', 'the'): ['signals'],\n",
       " ('from', 'the', 'signals'): ['it'],\n",
       " ('the', 'signals', 'it'): ['was'],\n",
       " ('signals', 'it', 'was'): ['getting'],\n",
       " ('it', 'was', 'getting'): ['from'],\n",
       " ('was', 'getting', 'from'): ['the'],\n",
       " ('getting', 'from', 'the'): ['frontal'],\n",
       " ('from', 'the', 'frontal'): ['lobes'],\n",
       " ('the', 'frontal', 'lobes'): ['…'],\n",
       " ('frontal', 'lobes', '…'): ['and'],\n",
       " ('lobes', '…', 'and'): ['the'],\n",
       " ('…', 'and', 'the'): ['middle-brain,'],\n",
       " ('and', 'the', 'middle-brain,'): ['meanwhile,'],\n",
       " ('the', 'middle-brain,', 'meanwhile,'): ['would'],\n",
       " ('middle-brain,', 'meanwhile,', 'would'): ['be'],\n",
       " ('meanwhile,', 'would', 'be'): ['trying'],\n",
       " ('would', 'be', 'trying'): ['desperately'],\n",
       " ('be', 'trying', 'desperately'): ['to'],\n",
       " ('trying', 'desperately', 'to'): ['put'],\n",
       " ('desperately', 'to', 'put'): ['a'],\n",
       " ('to', 'put', 'a'): ['different'],\n",
       " ('put', 'a', 'different'): ['interpretation'],\n",
       " ('a', 'different', 'interpretation'): ['on'],\n",
       " ('different', 'interpretation', 'on'): ['the'],\n",
       " ('interpretation', 'on', 'the'): ['scene,'],\n",
       " ('on', 'the', 'scene,'): ['before'],\n",
       " ('the', 'scene,', 'before'): ['passing'],\n",
       " ('scene,', 'before', 'passing'): ['it'],\n",
       " ('before', 'passing', 'it'): ['back'],\n",
       " ('passing', 'it', 'back'): ['to'],\n",
       " ('it', 'back', 'to'): ['the'],\n",
       " ('back', 'to', 'the'): ['medulla'],\n",
       " ('to', 'the', 'medulla'): ['and'],\n",
       " ('the', 'medulla', 'and'): ['the'],\n",
       " ('medulla', 'and', 'the'): ['risk'],\n",
       " ('and', 'the', 'risk'): ['of'],\n",
       " ('the', 'risk', 'of'): ['physical'],\n",
       " ('risk', 'of', 'physical'): ['action.'],\n",
       " ('of', 'physical', 'action.'): ['Acid'],\n",
       " ('physical', 'action.', 'Acid'): ['is'],\n",
       " ('action.', 'Acid', 'is'): ['a'],\n",
       " ('Acid', 'is', 'a'): ['relatively'],\n",
       " ('is', 'a', 'relatively'): ['complex'],\n",
       " ('a', 'relatively', 'complex'): ['drug,'],\n",
       " ('relatively', 'complex', 'drug,'): ['in'],\n",
       " ('complex', 'drug,', 'in'): ['its'],\n",
       " ('drug,', 'in', 'its'): ['effects,'],\n",
       " ('in', 'its', 'effects,'): ['while'],\n",
       " ('its', 'effects,', 'while'): ['mescaline'],\n",
       " ('effects,', 'while', 'mescaline'): ['is'],\n",
       " ('while', 'mescaline', 'is'): ['pretty'],\n",
       " ('mescaline', 'is', 'pretty'): ['simple'],\n",
       " ('is', 'pretty', 'simple'): ['and'],\n",
       " ('pretty', 'simple', 'and'): ['straightforward'],\n",
       " ('simple', 'and', 'straightforward'): ['–'],\n",
       " ('and', 'straightforward', '–'): ['but'],\n",
       " ('straightforward', '–', 'but'): ['in'],\n",
       " ('–', 'but', 'in'): ['a'],\n",
       " ('but', 'in', 'a'): ['scene'],\n",
       " ('in', 'a', 'scene'): ['like'],\n",
       " ('a', 'scene', 'like'): ['this,'],\n",
       " ('scene', 'like', 'this,'): ['the'],\n",
       " ('like', 'this,', 'the'): ['difference'],\n",
       " ('this,', 'the', 'difference'): ['was'],\n",
       " ('the', 'difference', 'was'): ['academic.'],\n",
       " ('difference', 'was', 'academic.'): ['There'],\n",
       " ('was', 'academic.', 'There'): ['was'],\n",
       " ('academic.', 'There', 'was'): ['simply'],\n",
       " ('There', 'was', 'simply'): ['no'],\n",
       " ('was', 'simply', 'no'): ['call,'],\n",
       " ('simply', 'no', 'call,'): ['at'],\n",
       " ('no', 'call,', 'at'): ['this'],\n",
       " ('call,', 'at', 'this'): ['conference,'],\n",
       " ('at', 'this', 'conference,'): ['for'],\n",
       " ('this', 'conference,', 'for'): ['anything'],\n",
       " ('conference,', 'for', 'anything'): ['but'],\n",
       " ('for', 'anything', 'but'): ['a'],\n",
       " ('anything', 'but', 'a'): ['massive'],\n",
       " ('but', 'a', 'massive'): ['consumption'],\n",
       " ('a', 'massive', 'consumption'): ['of'],\n",
       " ('massive', 'consumption', 'of'): ['Downers:'],\n",
       " ('consumption', 'of', 'Downers:'): ['Reds,'],\n",
       " ('of', 'Downers:', 'Reds,'): ['Grass'],\n",
       " ('Downers:', 'Reds,', 'Grass'): ['and'],\n",
       " ('Reds,', 'Grass', 'and'): ['Booze,'],\n",
       " ('Grass', 'and', 'Booze,'): ['because'],\n",
       " ('and', 'Booze,', 'because'): ['the'],\n",
       " ('Booze,', 'because', 'the'): ['whole'],\n",
       " ('because', 'the', 'whole'): ['program'],\n",
       " ('the', 'whole', 'program'): ['had'],\n",
       " ('whole', 'program', 'had'): ['apparently'],\n",
       " ('program', 'had', 'apparently'): ['been'],\n",
       " ('had', 'apparently', 'been'): ['set'],\n",
       " ('apparently', 'been', 'set'): ['up'],\n",
       " ('been', 'set', 'up'): ['by'],\n",
       " ('set', 'up', 'by'): ['people'],\n",
       " ('up', 'by', 'people'): ['who'],\n",
       " ('by', 'people', 'who'): ['had'],\n",
       " ('people', 'who', 'had'): ['been'],\n",
       " ('who', 'had', 'been'): ['in'],\n",
       " ('had', 'been', 'in'): ['a'],\n",
       " ('been', 'in', 'a'): ['Seconal'],\n",
       " ('in', 'a', 'Seconal'): ['stupor'],\n",
       " ('a', 'Seconal', 'stupor'): ['since'],\n",
       " ('Seconal', 'stupor', 'since'): ['1964.'],\n",
       " ('stupor', 'since', '1964.'): [None]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg143_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def gen_from_model(n, model, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_item = random.choice(model[start])\n",
    "        if next_item is None:\n",
    "            break\n",
    "        else:\n",
    "            output.append(next_item)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg143_word_model = markov_model(2, open(\"pg143.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a sensual/surface drug that exaggerates reality, instead of altering it – but in a scene like this, the difference was academic. There was simply no call, at this conference, for anything but a massive consumption of Downers: Reds, Grass and Booze, because the whole program had apparently been set up by people who had been in a scene like this, the difference was academic. There was simply no call, at this conference, for anything but a massive consumption of Downers: Reds, Grass and Booze, because the whole thing on acid … except for some of the people; there were faces and bodies\n"
     ]
    }
   ],
   "source": [
    "generated_words = gen_from_model(2, pg143_word_model)\n",
    "print(' '.join(generated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = open(\"pg143.txt\").read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_b = open(\"antidrug.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = open(\"pg143.txt\").read()\n",
    "words_1 = text_1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_1 = [(words_1[i], words_1[i+1]) for i in range(len(words_1)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_1 = []\n",
    "for i in range(len(words_1)-1):\n",
    "    this_pair = (words_1[i], words_1[i+1])\n",
    "    pairs_1.append(this_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'suspect'),\n",
       " ('suspect', 'we'),\n",
       " ('we', 'could'),\n",
       " ('could', 'have'),\n",
       " ('have', 'done'),\n",
       " ('done', 'the'),\n",
       " ('the', 'whole'),\n",
       " ('whole', 'thing'),\n",
       " ('thing', 'on'),\n",
       " ('on', 'acid'),\n",
       " ('acid', '…'),\n",
       " ('…', 'except'),\n",
       " ('except', 'for'),\n",
       " ('for', 'some'),\n",
       " ('some', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'people;'),\n",
       " ('people;', 'there'),\n",
       " ('there', 'were'),\n",
       " ('were', 'faces'),\n",
       " ('faces', 'and'),\n",
       " ('and', 'bodies'),\n",
       " ('bodies', 'in'),\n",
       " ('in', 'that'),\n",
       " ('that', 'group'),\n",
       " ('group', 'who'),\n",
       " ('who', 'would'),\n",
       " ('would', 'have'),\n",
       " ('have', 'been'),\n",
       " ('been', 'absolutely'),\n",
       " ('absolutely', 'unendurable'),\n",
       " ('unendurable', 'on'),\n",
       " ('on', 'acid.'),\n",
       " ('acid.', 'The'),\n",
       " ('The', 'sight'),\n",
       " ('sight', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', '355-pound'),\n",
       " ('355-pound', 'police'),\n",
       " ('police', 'chief'),\n",
       " ('chief', 'from'),\n",
       " ('from', 'Waco,'),\n",
       " ('Waco,', 'Texas,'),\n",
       " ('Texas,', 'necking'),\n",
       " ('necking', 'openly'),\n",
       " ('openly', 'with'),\n",
       " ('with', 'his'),\n",
       " ('his', '290-pound'),\n",
       " ('290-pound', 'wife'),\n",
       " ('wife', '(or')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_1[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = open(\"antidrug.txt\").read()\n",
    "words_2 = text_2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_2 = [(words_2[i], words_2[i+1]) for i in range (len(words_2)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_2 = []\n",
    "for i in range(len(words_2)-1):\n",
    "    that_pair = (words_2[i], words_2[i+1])\n",
    "    pairs_2.append(that_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Angel', 'dust'),\n",
       " ('dust', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'disgust.'),\n",
       " ('disgust.', 'Be'),\n",
       " ('Be', 'a'),\n",
       " ('a', 'scholar'),\n",
       " ('scholar', 'instead'),\n",
       " ('instead', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'pot'),\n",
       " ('pot', 'head.'),\n",
       " ('head.', 'Be'),\n",
       " ('Be', 'all'),\n",
       " ('all', 'you'),\n",
       " ('you', 'can'),\n",
       " ('can', 'be,'),\n",
       " ('be,', 'Go'),\n",
       " ('Go', 'drug'),\n",
       " ('drug', 'free.'),\n",
       " ('free.', 'Be'),\n",
       " ('Be', 'proud'),\n",
       " ('proud', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'drug'),\n",
       " ('drug', 'free.'),\n",
       " ('free.', 'Blunts'),\n",
       " ('Blunts', 'are'),\n",
       " ('are', 'bad.'),\n",
       " ('bad.', 'Car'),\n",
       " ('Car', 'pipes'),\n",
       " ('pipes', 'over'),\n",
       " ('over', 'crack'),\n",
       " ('crack', 'pipes.'),\n",
       " ('pipes.', 'Choose'),\n",
       " ('Choose', 'life'),\n",
       " ('life', 'over'),\n",
       " ('over', 'drugs.'),\n",
       " ('drugs.', 'Choose'),\n",
       " ('Choose', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'jolly,'),\n",
       " ('jolly,', 'stay'),\n",
       " ('stay', 'away'),\n",
       " ('away', 'from'),\n",
       " ('from', 'Molly.'),\n",
       " ('Molly.', 'Cocaine'),\n",
       " ('Cocaine', 'is'),\n",
       " ('is', 'just'),\n",
       " ('just', 'insane.')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_2[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts1 = Counter(pairs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'a'), 3),\n",
       " (('the', 'whole'), 2),\n",
       " (('sight', 'of'), 2),\n",
       " (('–', 'but'), 2),\n",
       " (('from', 'the'), 2),\n",
       " (('and', 'the'), 2),\n",
       " (('I', 'suspect'), 1),\n",
       " (('suspect', 'we'), 1),\n",
       " (('we', 'could'), 1),\n",
       " (('could', 'have'), 1),\n",
       " (('have', 'done'), 1),\n",
       " (('done', 'the'), 1),\n",
       " (('whole', 'thing'), 1),\n",
       " (('thing', 'on'), 1),\n",
       " (('on', 'acid'), 1),\n",
       " (('acid', '…'), 1),\n",
       " (('…', 'except'), 1),\n",
       " (('except', 'for'), 1),\n",
       " (('for', 'some'), 1),\n",
       " (('some', 'of'), 1),\n",
       " (('of', 'the'), 1),\n",
       " (('the', 'people;'), 1),\n",
       " (('people;', 'there'), 1),\n",
       " (('there', 'were'), 1),\n",
       " (('were', 'faces'), 1),\n",
       " (('faces', 'and'), 1),\n",
       " (('and', 'bodies'), 1),\n",
       " (('bodies', 'in'), 1),\n",
       " (('in', 'that'), 1),\n",
       " (('that', 'group'), 1),\n",
       " (('group', 'who'), 1),\n",
       " (('who', 'would'), 1),\n",
       " (('would', 'have'), 1),\n",
       " (('have', 'been'), 1),\n",
       " (('been', 'absolutely'), 1),\n",
       " (('absolutely', 'unendurable'), 1),\n",
       " (('unendurable', 'on'), 1),\n",
       " (('on', 'acid.'), 1),\n",
       " (('acid.', 'The'), 1),\n",
       " (('The', 'sight'), 1),\n",
       " (('of', 'a'), 1),\n",
       " (('a', '355-pound'), 1),\n",
       " (('355-pound', 'police'), 1),\n",
       " (('police', 'chief'), 1),\n",
       " (('chief', 'from'), 1),\n",
       " (('from', 'Waco,'), 1),\n",
       " (('Waco,', 'Texas,'), 1),\n",
       " (('Texas,', 'necking'), 1),\n",
       " (('necking', 'openly'), 1),\n",
       " (('openly', 'with'), 1)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_counts1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts2 = Counter(pairs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('is', 'a'), 4),\n",
       " (('is', 'for'), 3),\n",
       " (('drug', 'free.'), 2),\n",
       " (('to', 'be'), 2),\n",
       " (('drugs.', 'Don’t'), 2),\n",
       " (('for', 'me.'), 2),\n",
       " (('have', 'to'), 2),\n",
       " (('is', 'no'), 2),\n",
       " (('Angel', 'dust'), 1),\n",
       " (('dust', 'is'), 1),\n",
       " (('a', 'disgust.'), 1),\n",
       " (('disgust.', 'Be'), 1),\n",
       " (('Be', 'a'), 1),\n",
       " (('a', 'scholar'), 1),\n",
       " (('scholar', 'instead'), 1),\n",
       " (('instead', 'of'), 1),\n",
       " (('of', 'a'), 1),\n",
       " (('a', 'pot'), 1),\n",
       " (('pot', 'head.'), 1),\n",
       " (('head.', 'Be'), 1),\n",
       " (('Be', 'all'), 1),\n",
       " (('all', 'you'), 1),\n",
       " (('you', 'can'), 1),\n",
       " (('can', 'be,'), 1),\n",
       " (('be,', 'Go'), 1),\n",
       " (('Go', 'drug'), 1),\n",
       " (('free.', 'Be'), 1),\n",
       " (('Be', 'proud'), 1),\n",
       " (('proud', 'to'), 1),\n",
       " (('be', 'drug'), 1),\n",
       " (('free.', 'Blunts'), 1),\n",
       " (('Blunts', 'are'), 1),\n",
       " (('are', 'bad.'), 1),\n",
       " (('bad.', 'Car'), 1),\n",
       " (('Car', 'pipes'), 1),\n",
       " (('pipes', 'over'), 1),\n",
       " (('over', 'crack'), 1),\n",
       " (('crack', 'pipes.'), 1),\n",
       " (('pipes.', 'Choose'), 1),\n",
       " (('Choose', 'life'), 1),\n",
       " (('life', 'over'), 1),\n",
       " (('over', 'drugs.'), 1),\n",
       " (('drugs.', 'Choose'), 1),\n",
       " (('Choose', 'to'), 1),\n",
       " (('be', 'jolly,'), 1),\n",
       " (('jolly,', 'stay'), 1),\n",
       " (('stay', 'away'), 1),\n",
       " (('away', 'from'), 1),\n",
       " (('from', 'Molly.'), 1),\n",
       " (('Molly.', 'Cocaine'), 1)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_counts2.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_grams = [tuple(words_1[i:i+7]) for i in range(len(words_1)-6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'suspect', 'we', 'could', 'have', 'done', 'the'),\n",
       " ('suspect', 'we', 'could', 'have', 'done', 'the', 'whole'),\n",
       " ('we', 'could', 'have', 'done', 'the', 'whole', 'thing'),\n",
       " ('could', 'have', 'done', 'the', 'whole', 'thing', 'on'),\n",
       " ('have', 'done', 'the', 'whole', 'thing', 'on', 'acid'),\n",
       " ('done', 'the', 'whole', 'thing', 'on', 'acid', '…'),\n",
       " ('the', 'whole', 'thing', 'on', 'acid', '…', 'except'),\n",
       " ('whole', 'thing', 'on', 'acid', '…', 'except', 'for'),\n",
       " ('thing', 'on', 'acid', '…', 'except', 'for', 'some'),\n",
       " ('on', 'acid', '…', 'except', 'for', 'some', 'of'),\n",
       " ('acid', '…', 'except', 'for', 'some', 'of', 'the'),\n",
       " ('…', 'except', 'for', 'some', 'of', 'the', 'people;'),\n",
       " ('except', 'for', 'some', 'of', 'the', 'people;', 'there'),\n",
       " ('for', 'some', 'of', 'the', 'people;', 'there', 'were'),\n",
       " ('some', 'of', 'the', 'people;', 'there', 'were', 'faces'),\n",
       " ('of', 'the', 'people;', 'there', 'were', 'faces', 'and'),\n",
       " ('the', 'people;', 'there', 'were', 'faces', 'and', 'bodies'),\n",
       " ('people;', 'there', 'were', 'faces', 'and', 'bodies', 'in'),\n",
       " ('there', 'were', 'faces', 'and', 'bodies', 'in', 'that'),\n",
       " ('were', 'faces', 'and', 'bodies', 'in', 'that', 'group')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_grams[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_grams = [tuple(words_2[i:i+6]) for i in range(len(words_2)-5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Angel', 'dust', 'is', 'a', 'disgust.', 'Be'),\n",
       " ('dust', 'is', 'a', 'disgust.', 'Be', 'a'),\n",
       " ('is', 'a', 'disgust.', 'Be', 'a', 'scholar'),\n",
       " ('a', 'disgust.', 'Be', 'a', 'scholar', 'instead'),\n",
       " ('disgust.', 'Be', 'a', 'scholar', 'instead', 'of'),\n",
       " ('Be', 'a', 'scholar', 'instead', 'of', 'a'),\n",
       " ('a', 'scholar', 'instead', 'of', 'a', 'pot'),\n",
       " ('scholar', 'instead', 'of', 'a', 'pot', 'head.'),\n",
       " ('instead', 'of', 'a', 'pot', 'head.', 'Be'),\n",
       " ('of', 'a', 'pot', 'head.', 'Be', 'all'),\n",
       " ('a', 'pot', 'head.', 'Be', 'all', 'you'),\n",
       " ('pot', 'head.', 'Be', 'all', 'you', 'can'),\n",
       " ('head.', 'Be', 'all', 'you', 'can', 'be,'),\n",
       " ('Be', 'all', 'you', 'can', 'be,', 'Go'),\n",
       " ('all', 'you', 'can', 'be,', 'Go', 'drug'),\n",
       " ('you', 'can', 'be,', 'Go', 'drug', 'free.'),\n",
       " ('can', 'be,', 'Go', 'drug', 'free.', 'Be'),\n",
       " ('be,', 'Go', 'drug', 'free.', 'Be', 'proud'),\n",
       " ('Go', 'drug', 'free.', 'Be', 'proud', 'to'),\n",
       " ('drug', 'free.', 'Be', 'proud', 'to', 'be')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six_grams[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_for_sequence(n, seq):\n",
    "    return [tuple(seq[i:i+n]) for i in range(len(seq)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('but', 'with', 'a', 'head', 'full'),\n",
       " ('fantastically', 'obese', 'human', 'beings', 'far'),\n",
       " ('–', 'but', 'in', 'a', 'scene'),\n",
       " ('whole', 'thing', 'on', 'acid', '…'),\n",
       " ('the', 'difference', 'was', 'academic.', 'There'),\n",
       " ('that', 'exaggerates', 'reality,', 'instead', 'of'),\n",
       " ('from', 'the', 'frontal', 'lobes', '…'),\n",
       " ('wife', '(or', 'whatever', 'woman', 'he'),\n",
       " ('trying', 'desperately', 'to', 'put', 'a'),\n",
       " ('There', 'was', 'simply', 'no', 'call,'),\n",
       " ('medulla', 'would', 'attempt', 'to', 'close'),\n",
       " ('–', 'which', 'is', 'mainly', 'a'),\n",
       " ('in', 'a', 'scene', 'like', 'this,'),\n",
       " ('effects,', 'while', 'mescaline', 'is', 'pretty'),\n",
       " ('beings', 'far', 'gone', 'in', 'a'),\n",
       " ('frontal', 'lobes', '…', 'and', 'the'),\n",
       " ('simple', 'and', 'straightforward', '–', 'but'),\n",
       " ('is', 'a', 'relatively', 'complex', 'drug,'),\n",
       " ('about', 'the', '“dangers', 'of', 'marijuana”'),\n",
       " ('a', 'movie', 'about', 'the', '“dangers')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg143_5grams = ngrams_for_sequence(5, open(\"pg143.txt\").read().split())\n",
    "random.sample(pg143_5grams, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food,', 'BAD', 'for', 'the', 'brain.'),\n",
       " ('If', 'I', 'must', 'say', 'so,'),\n",
       " ('to', '“The', 'Chronic”', 'don’t', 'go'),\n",
       " ('wasted', 'is', 'a', 'waste', 'of'),\n",
       " ('than', 'the', 'meth', 'lab.', 'The'),\n",
       " ('The', 'ZigZag', 'is', 'bad.', 'There'),\n",
       " ('REAL', 'tooth', 'fairy.', 'More', 'hugs'),\n",
       " ('You', 'are', 'bound', 'to', 'fall'),\n",
       " ('to', 'use', 'drugs.', 'Don’t', 'pass'),\n",
       " ('if', 'you', 'smoke', 'a', 'meth'),\n",
       " ('meth', 'leads', 'to', 'death.', 'Smoking’'),\n",
       " ('rock.', 'Don’t', 'let', '“John', 'Blaze”'),\n",
       " ('coke', 'I', 'do', 'is', 'Diet.'),\n",
       " ('Smoking', 'meth', 'leads', 'to', 'death.'),\n",
       " ('ball.', 'You’ll', 'never', 'be', 'as'),\n",
       " ('proud', 'to', 'be', 'drug', 'free.'),\n",
       " ('and', 'drugs', 'don’t', 'mix.', 'Flee'),\n",
       " ('a', 'phase.', 'Don’t', 'make', 'excuses'),\n",
       " ('blunt', 'is', 'for', 'chumps.', 'Say'),\n",
       " ('Getting', 'wasted', 'is', 'a', 'waste')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antidrug_5grams = ngrams_for_sequence(5, open(\"antidrug.txt\").read().split())\n",
    "random.sample(antidrug_5grams, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_model(model, n, seq):\n",
    "    # make a copy of seq and append None to the end\n",
    "    seq = list(seq[:]) + [None]\n",
    "    for i in range(len(seq)-n):\n",
    "        # tuple because we're using it as a dict key!\n",
    "        gram = tuple(seq[i:i+n])\n",
    "        next_item = seq[i+n]            \n",
    "        if gram not in model:\n",
    "            model[gram] = []\n",
    "        model[gram].append(next_item)\n",
    "\n",
    "def markov_model(n, seq):\n",
    "    model = {}\n",
    "    add_to_model(model, n, seq)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg143_markov_model = markov_model(3, open(\"pg143.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('I', 'suspect', 'we'): ['could'],\n",
       " ('suspect', 'we', 'could'): ['have'],\n",
       " ('we', 'could', 'have'): ['done'],\n",
       " ('could', 'have', 'done'): ['the'],\n",
       " ('have', 'done', 'the'): ['whole'],\n",
       " ('done', 'the', 'whole'): ['thing'],\n",
       " ('the', 'whole', 'thing'): ['on'],\n",
       " ('whole', 'thing', 'on'): ['acid'],\n",
       " ('thing', 'on', 'acid'): ['…'],\n",
       " ('on', 'acid', '…'): ['except'],\n",
       " ('acid', '…', 'except'): ['for'],\n",
       " ('…', 'except', 'for'): ['some'],\n",
       " ('except', 'for', 'some'): ['of'],\n",
       " ('for', 'some', 'of'): ['the'],\n",
       " ('some', 'of', 'the'): ['people;'],\n",
       " ('of', 'the', 'people;'): ['there'],\n",
       " ('the', 'people;', 'there'): ['were'],\n",
       " ('people;', 'there', 'were'): ['faces'],\n",
       " ('there', 'were', 'faces'): ['and'],\n",
       " ('were', 'faces', 'and'): ['bodies'],\n",
       " ('faces', 'and', 'bodies'): ['in'],\n",
       " ('and', 'bodies', 'in'): ['that'],\n",
       " ('bodies', 'in', 'that'): ['group'],\n",
       " ('in', 'that', 'group'): ['who'],\n",
       " ('that', 'group', 'who'): ['would'],\n",
       " ('group', 'who', 'would'): ['have'],\n",
       " ('who', 'would', 'have'): ['been'],\n",
       " ('would', 'have', 'been'): ['absolutely'],\n",
       " ('have', 'been', 'absolutely'): ['unendurable'],\n",
       " ('been', 'absolutely', 'unendurable'): ['on'],\n",
       " ('absolutely', 'unendurable', 'on'): ['acid.'],\n",
       " ('unendurable', 'on', 'acid.'): ['The'],\n",
       " ('on', 'acid.', 'The'): ['sight'],\n",
       " ('acid.', 'The', 'sight'): ['of'],\n",
       " ('The', 'sight', 'of'): ['a'],\n",
       " ('sight', 'of', 'a'): ['355-pound'],\n",
       " ('of', 'a', '355-pound'): ['police'],\n",
       " ('a', '355-pound', 'police'): ['chief'],\n",
       " ('355-pound', 'police', 'chief'): ['from'],\n",
       " ('police', 'chief', 'from'): ['Waco,'],\n",
       " ('chief', 'from', 'Waco,'): ['Texas,'],\n",
       " ('from', 'Waco,', 'Texas,'): ['necking'],\n",
       " ('Waco,', 'Texas,', 'necking'): ['openly'],\n",
       " ('Texas,', 'necking', 'openly'): ['with'],\n",
       " ('necking', 'openly', 'with'): ['his'],\n",
       " ('openly', 'with', 'his'): ['290-pound'],\n",
       " ('with', 'his', '290-pound'): ['wife'],\n",
       " ('his', '290-pound', 'wife'): ['(or'],\n",
       " ('290-pound', 'wife', '(or'): ['whatever'],\n",
       " ('wife', '(or', 'whatever'): ['woman'],\n",
       " ('(or', 'whatever', 'woman'): ['he'],\n",
       " ('whatever', 'woman', 'he'): ['had'],\n",
       " ('woman', 'he', 'had'): ['with'],\n",
       " ('he', 'had', 'with'): ['him)'],\n",
       " ('had', 'with', 'him)'): ['when'],\n",
       " ('with', 'him)', 'when'): ['the'],\n",
       " ('him)', 'when', 'the'): ['lights'],\n",
       " ('when', 'the', 'lights'): ['were'],\n",
       " ('the', 'lights', 'were'): ['turned'],\n",
       " ('lights', 'were', 'turned'): ['off'],\n",
       " ('were', 'turned', 'off'): ['for'],\n",
       " ('turned', 'off', 'for'): ['a'],\n",
       " ('off', 'for', 'a'): ['Dope'],\n",
       " ('for', 'a', 'Dope'): ['Film'],\n",
       " ('a', 'Dope', 'Film'): ['was'],\n",
       " ('Dope', 'Film', 'was'): ['just'],\n",
       " ('Film', 'was', 'just'): ['barely'],\n",
       " ('was', 'just', 'barely'): ['tolerable'],\n",
       " ('just', 'barely', 'tolerable'): ['on'],\n",
       " ('barely', 'tolerable', 'on'): ['mescaline'],\n",
       " ('tolerable', 'on', 'mescaline'): ['–'],\n",
       " ('on', 'mescaline', '–'): ['which'],\n",
       " ('mescaline', '–', 'which'): ['is'],\n",
       " ('–', 'which', 'is'): ['mainly'],\n",
       " ('which', 'is', 'mainly'): ['a'],\n",
       " ('is', 'mainly', 'a'): ['sensual/surface'],\n",
       " ('mainly', 'a', 'sensual/surface'): ['drug'],\n",
       " ('a', 'sensual/surface', 'drug'): ['that'],\n",
       " ('sensual/surface', 'drug', 'that'): ['exaggerates'],\n",
       " ('drug', 'that', 'exaggerates'): ['reality,'],\n",
       " ('that', 'exaggerates', 'reality,'): ['instead'],\n",
       " ('exaggerates', 'reality,', 'instead'): ['of'],\n",
       " ('reality,', 'instead', 'of'): ['altering'],\n",
       " ('instead', 'of', 'altering'): ['it'],\n",
       " ('of', 'altering', 'it'): ['–'],\n",
       " ('altering', 'it', '–'): ['but'],\n",
       " ('it', '–', 'but'): ['with'],\n",
       " ('–', 'but', 'with'): ['a'],\n",
       " ('but', 'with', 'a'): ['head'],\n",
       " ('with', 'a', 'head'): ['full'],\n",
       " ('a', 'head', 'full'): ['of'],\n",
       " ('head', 'full', 'of'): ['acid,'],\n",
       " ('full', 'of', 'acid,'): ['the'],\n",
       " ('of', 'acid,', 'the'): ['sight'],\n",
       " ('acid,', 'the', 'sight'): ['of'],\n",
       " ('the', 'sight', 'of'): ['two'],\n",
       " ('sight', 'of', 'two'): ['fantastically'],\n",
       " ('of', 'two', 'fantastically'): ['obese'],\n",
       " ('two', 'fantastically', 'obese'): ['human'],\n",
       " ('fantastically', 'obese', 'human'): ['beings'],\n",
       " ('obese', 'human', 'beings'): ['far'],\n",
       " ('human', 'beings', 'far'): ['gone'],\n",
       " ('beings', 'far', 'gone'): ['in'],\n",
       " ('far', 'gone', 'in'): ['a'],\n",
       " ('gone', 'in', 'a'): ['public'],\n",
       " ('in', 'a', 'public'): ['grope'],\n",
       " ('a', 'public', 'grope'): ['while'],\n",
       " ('public', 'grope', 'while'): ['a'],\n",
       " ('grope', 'while', 'a'): ['thousand'],\n",
       " ('while', 'a', 'thousand'): ['cops'],\n",
       " ('a', 'thousand', 'cops'): ['all'],\n",
       " ('thousand', 'cops', 'all'): ['around'],\n",
       " ('cops', 'all', 'around'): ['them'],\n",
       " ('all', 'around', 'them'): ['watched'],\n",
       " ('around', 'them', 'watched'): ['a'],\n",
       " ('them', 'watched', 'a'): ['movie'],\n",
       " ('watched', 'a', 'movie'): ['about'],\n",
       " ('a', 'movie', 'about'): ['the'],\n",
       " ('movie', 'about', 'the'): ['“dangers'],\n",
       " ('about', 'the', '“dangers'): ['of'],\n",
       " ('the', '“dangers', 'of'): ['marijuana”'],\n",
       " ('“dangers', 'of', 'marijuana”'): ['would'],\n",
       " ('of', 'marijuana”', 'would'): ['not'],\n",
       " ('marijuana”', 'would', 'not'): ['be'],\n",
       " ('would', 'not', 'be'): ['emotionally'],\n",
       " ('not', 'be', 'emotionally'): ['acceptable.'],\n",
       " ('be', 'emotionally', 'acceptable.'): ['The'],\n",
       " ('emotionally', 'acceptable.', 'The'): ['brain'],\n",
       " ('acceptable.', 'The', 'brain'): ['would'],\n",
       " ('The', 'brain', 'would'): ['reject'],\n",
       " ('brain', 'would', 'reject'): ['it:'],\n",
       " ('would', 'reject', 'it:'): ['The'],\n",
       " ('reject', 'it:', 'The'): ['medulla'],\n",
       " ('it:', 'The', 'medulla'): ['would'],\n",
       " ('The', 'medulla', 'would'): ['attempt'],\n",
       " ('medulla', 'would', 'attempt'): ['to'],\n",
       " ('would', 'attempt', 'to'): ['close'],\n",
       " ('attempt', 'to', 'close'): ['itself'],\n",
       " ('to', 'close', 'itself'): ['off'],\n",
       " ('close', 'itself', 'off'): ['from'],\n",
       " ('itself', 'off', 'from'): ['the'],\n",
       " ('off', 'from', 'the'): ['signals'],\n",
       " ('from', 'the', 'signals'): ['it'],\n",
       " ('the', 'signals', 'it'): ['was'],\n",
       " ('signals', 'it', 'was'): ['getting'],\n",
       " ('it', 'was', 'getting'): ['from'],\n",
       " ('was', 'getting', 'from'): ['the'],\n",
       " ('getting', 'from', 'the'): ['frontal'],\n",
       " ('from', 'the', 'frontal'): ['lobes'],\n",
       " ('the', 'frontal', 'lobes'): ['…'],\n",
       " ('frontal', 'lobes', '…'): ['and'],\n",
       " ('lobes', '…', 'and'): ['the'],\n",
       " ('…', 'and', 'the'): ['middle-brain,'],\n",
       " ('and', 'the', 'middle-brain,'): ['meanwhile,'],\n",
       " ('the', 'middle-brain,', 'meanwhile,'): ['would'],\n",
       " ('middle-brain,', 'meanwhile,', 'would'): ['be'],\n",
       " ('meanwhile,', 'would', 'be'): ['trying'],\n",
       " ('would', 'be', 'trying'): ['desperately'],\n",
       " ('be', 'trying', 'desperately'): ['to'],\n",
       " ('trying', 'desperately', 'to'): ['put'],\n",
       " ('desperately', 'to', 'put'): ['a'],\n",
       " ('to', 'put', 'a'): ['different'],\n",
       " ('put', 'a', 'different'): ['interpretation'],\n",
       " ('a', 'different', 'interpretation'): ['on'],\n",
       " ('different', 'interpretation', 'on'): ['the'],\n",
       " ('interpretation', 'on', 'the'): ['scene,'],\n",
       " ('on', 'the', 'scene,'): ['before'],\n",
       " ('the', 'scene,', 'before'): ['passing'],\n",
       " ('scene,', 'before', 'passing'): ['it'],\n",
       " ('before', 'passing', 'it'): ['back'],\n",
       " ('passing', 'it', 'back'): ['to'],\n",
       " ('it', 'back', 'to'): ['the'],\n",
       " ('back', 'to', 'the'): ['medulla'],\n",
       " ('to', 'the', 'medulla'): ['and'],\n",
       " ('the', 'medulla', 'and'): ['the'],\n",
       " ('medulla', 'and', 'the'): ['risk'],\n",
       " ('and', 'the', 'risk'): ['of'],\n",
       " ('the', 'risk', 'of'): ['physical'],\n",
       " ('risk', 'of', 'physical'): ['action.'],\n",
       " ('of', 'physical', 'action.'): ['Acid'],\n",
       " ('physical', 'action.', 'Acid'): ['is'],\n",
       " ('action.', 'Acid', 'is'): ['a'],\n",
       " ('Acid', 'is', 'a'): ['relatively'],\n",
       " ('is', 'a', 'relatively'): ['complex'],\n",
       " ('a', 'relatively', 'complex'): ['drug,'],\n",
       " ('relatively', 'complex', 'drug,'): ['in'],\n",
       " ('complex', 'drug,', 'in'): ['its'],\n",
       " ('drug,', 'in', 'its'): ['effects,'],\n",
       " ('in', 'its', 'effects,'): ['while'],\n",
       " ('its', 'effects,', 'while'): ['mescaline'],\n",
       " ('effects,', 'while', 'mescaline'): ['is'],\n",
       " ('while', 'mescaline', 'is'): ['pretty'],\n",
       " ('mescaline', 'is', 'pretty'): ['simple'],\n",
       " ('is', 'pretty', 'simple'): ['and'],\n",
       " ('pretty', 'simple', 'and'): ['straightforward'],\n",
       " ('simple', 'and', 'straightforward'): ['–'],\n",
       " ('and', 'straightforward', '–'): ['but'],\n",
       " ('straightforward', '–', 'but'): ['in'],\n",
       " ('–', 'but', 'in'): ['a'],\n",
       " ('but', 'in', 'a'): ['scene'],\n",
       " ('in', 'a', 'scene'): ['like'],\n",
       " ('a', 'scene', 'like'): ['this,'],\n",
       " ('scene', 'like', 'this,'): ['the'],\n",
       " ('like', 'this,', 'the'): ['difference'],\n",
       " ('this,', 'the', 'difference'): ['was'],\n",
       " ('the', 'difference', 'was'): ['academic.'],\n",
       " ('difference', 'was', 'academic.'): ['There'],\n",
       " ('was', 'academic.', 'There'): ['was'],\n",
       " ('academic.', 'There', 'was'): ['simply'],\n",
       " ('There', 'was', 'simply'): ['no'],\n",
       " ('was', 'simply', 'no'): ['call,'],\n",
       " ('simply', 'no', 'call,'): ['at'],\n",
       " ('no', 'call,', 'at'): ['this'],\n",
       " ('call,', 'at', 'this'): ['conference,'],\n",
       " ('at', 'this', 'conference,'): ['for'],\n",
       " ('this', 'conference,', 'for'): ['anything'],\n",
       " ('conference,', 'for', 'anything'): ['but'],\n",
       " ('for', 'anything', 'but'): ['a'],\n",
       " ('anything', 'but', 'a'): ['massive'],\n",
       " ('but', 'a', 'massive'): ['consumption'],\n",
       " ('a', 'massive', 'consumption'): ['of'],\n",
       " ('massive', 'consumption', 'of'): ['Downers:'],\n",
       " ('consumption', 'of', 'Downers:'): ['Reds,'],\n",
       " ('of', 'Downers:', 'Reds,'): ['Grass'],\n",
       " ('Downers:', 'Reds,', 'Grass'): ['and'],\n",
       " ('Reds,', 'Grass', 'and'): ['Booze,'],\n",
       " ('Grass', 'and', 'Booze,'): ['because'],\n",
       " ('and', 'Booze,', 'because'): ['the'],\n",
       " ('Booze,', 'because', 'the'): ['whole'],\n",
       " ('because', 'the', 'whole'): ['program'],\n",
       " ('the', 'whole', 'program'): ['had'],\n",
       " ('whole', 'program', 'had'): ['apparently'],\n",
       " ('program', 'had', 'apparently'): ['been'],\n",
       " ('had', 'apparently', 'been'): ['set'],\n",
       " ('apparently', 'been', 'set'): ['up'],\n",
       " ('been', 'set', 'up'): ['by'],\n",
       " ('set', 'up', 'by'): ['people'],\n",
       " ('up', 'by', 'people'): ['who'],\n",
       " ('by', 'people', 'who'): ['had'],\n",
       " ('people', 'who', 'had'): ['been'],\n",
       " ('who', 'had', 'been'): ['in'],\n",
       " ('had', 'been', 'in'): ['a'],\n",
       " ('been', 'in', 'a'): ['Seconal'],\n",
       " ('in', 'a', 'Seconal'): ['stupor'],\n",
       " ('a', 'Seconal', 'stupor'): ['since'],\n",
       " ('Seconal', 'stupor', 'since'): ['1964.'],\n",
       " ('stupor', 'since', '1964.'): [None]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg143_markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "antidrug_markov_model = markov_model(3, open(\"antidrug.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Angel', 'dust', 'is'): ['a'],\n",
       " ('dust', 'is', 'a'): ['disgust.'],\n",
       " ('is', 'a', 'disgust.'): ['Be'],\n",
       " ('a', 'disgust.', 'Be'): ['a'],\n",
       " ('disgust.', 'Be', 'a'): ['scholar'],\n",
       " ('Be', 'a', 'scholar'): ['instead'],\n",
       " ('a', 'scholar', 'instead'): ['of'],\n",
       " ('scholar', 'instead', 'of'): ['a'],\n",
       " ('instead', 'of', 'a'): ['pot'],\n",
       " ('of', 'a', 'pot'): ['head.'],\n",
       " ('a', 'pot', 'head.'): ['Be'],\n",
       " ('pot', 'head.', 'Be'): ['all'],\n",
       " ('head.', 'Be', 'all'): ['you'],\n",
       " ('Be', 'all', 'you'): ['can'],\n",
       " ('all', 'you', 'can'): ['be,'],\n",
       " ('you', 'can', 'be,'): ['Go'],\n",
       " ('can', 'be,', 'Go'): ['drug'],\n",
       " ('be,', 'Go', 'drug'): ['free.'],\n",
       " ('Go', 'drug', 'free.'): ['Be'],\n",
       " ('drug', 'free.', 'Be'): ['proud'],\n",
       " ('free.', 'Be', 'proud'): ['to'],\n",
       " ('Be', 'proud', 'to'): ['be'],\n",
       " ('proud', 'to', 'be'): ['drug'],\n",
       " ('to', 'be', 'drug'): ['free.'],\n",
       " ('be', 'drug', 'free.'): ['Blunts'],\n",
       " ('drug', 'free.', 'Blunts'): ['are'],\n",
       " ('free.', 'Blunts', 'are'): ['bad.'],\n",
       " ('Blunts', 'are', 'bad.'): ['Car'],\n",
       " ('are', 'bad.', 'Car'): ['pipes'],\n",
       " ('bad.', 'Car', 'pipes'): ['over'],\n",
       " ('Car', 'pipes', 'over'): ['crack'],\n",
       " ('pipes', 'over', 'crack'): ['pipes.'],\n",
       " ('over', 'crack', 'pipes.'): ['Choose'],\n",
       " ('crack', 'pipes.', 'Choose'): ['life'],\n",
       " ('pipes.', 'Choose', 'life'): ['over'],\n",
       " ('Choose', 'life', 'over'): ['drugs.'],\n",
       " ('life', 'over', 'drugs.'): ['Choose'],\n",
       " ('over', 'drugs.', 'Choose'): ['to'],\n",
       " ('drugs.', 'Choose', 'to'): ['be'],\n",
       " ('Choose', 'to', 'be'): ['jolly,'],\n",
       " ('to', 'be', 'jolly,'): ['stay'],\n",
       " ('be', 'jolly,', 'stay'): ['away'],\n",
       " ('jolly,', 'stay', 'away'): ['from'],\n",
       " ('stay', 'away', 'from'): ['Molly.'],\n",
       " ('away', 'from', 'Molly.'): ['Cocaine'],\n",
       " ('from', 'Molly.', 'Cocaine'): ['is'],\n",
       " ('Molly.', 'Cocaine', 'is'): ['just'],\n",
       " ('Cocaine', 'is', 'just'): ['insane.'],\n",
       " ('is', 'just', 'insane.'): ['Crack'],\n",
       " ('just', 'insane.', 'Crack'): ['is'],\n",
       " ('insane.', 'Crack', 'is'): ['super'],\n",
       " ('Crack', 'is', 'super'): ['whack.'],\n",
       " ('is', 'super', 'whack.'): ['Do'],\n",
       " ('super', 'whack.', 'Do'): ['not'],\n",
       " ('whack.', 'Do', 'not'): ['blow'],\n",
       " ('Do', 'not', 'blow'): ['the'],\n",
       " ('not', 'blow', 'the'): ['snow.'],\n",
       " ('blow', 'the', 'snow.'): ['Don’t'],\n",
       " ('the', 'snow.', 'Don’t'): ['approach'],\n",
       " ('snow.', 'Don’t', 'approach'): ['the'],\n",
       " ('Don’t', 'approach', 'the'): ['roach.'],\n",
       " ('approach', 'the', 'roach.'): ['Don’t'],\n",
       " ('the', 'roach.', 'Don’t'): ['be'],\n",
       " ('roach.', 'Don’t', 'be'): ['a'],\n",
       " ('Don’t', 'be', 'a'): ['flunky'],\n",
       " ('be', 'a', 'flunky'): ['or'],\n",
       " ('a', 'flunky', 'or'): ['a'],\n",
       " ('flunky', 'or', 'a'): ['junkie,'],\n",
       " ('or', 'a', 'junkie,'): ['say'],\n",
       " ('a', 'junkie,', 'say'): ['no'],\n",
       " ('junkie,', 'say', 'no'): ['to'],\n",
       " ('say', 'no', 'to'): ['drugs.'],\n",
       " ('no', 'to', 'drugs.'): ['Don’t'],\n",
       " ('to', 'drugs.', 'Don’t'): ['get'],\n",
       " ('drugs.', 'Don’t', 'get'): ['blasted'],\n",
       " ('Don’t', 'get', 'blasted'): ['unless'],\n",
       " ('get', 'blasted', 'unless'): ['you’re'],\n",
       " ('blasted', 'unless', 'you’re'): ['playing'],\n",
       " ('unless', 'you’re', 'playing'): ['laser'],\n",
       " ('you’re', 'playing', 'laser'): ['tag.'],\n",
       " ('playing', 'laser', 'tag.'): ['Don’t'],\n",
       " ('laser', 'tag.', 'Don’t'): ['hit'],\n",
       " ('tag.', 'Don’t', 'hit'): ['the'],\n",
       " ('Don’t', 'hit', 'the'): ['block'],\n",
       " ('hit', 'the', 'block'): ['for'],\n",
       " ('the', 'block', 'for'): ['a'],\n",
       " ('block', 'for', 'a'): ['crack'],\n",
       " ('for', 'a', 'crack'): ['rock.'],\n",
       " ('a', 'crack', 'rock.'): ['Don’t'],\n",
       " ('crack', 'rock.', 'Don’t'): ['let'],\n",
       " ('rock.', 'Don’t', 'let'): ['“John'],\n",
       " ('Don’t', 'let', '“John'): ['Blaze”'],\n",
       " ('let', '“John', 'Blaze”'): ['become'],\n",
       " ('“John', 'Blaze”', 'become'): ['a'],\n",
       " ('Blaze”', 'become', 'a'): ['phase.'],\n",
       " ('become', 'a', 'phase.'): ['Don’t'],\n",
       " ('a', 'phase.', 'Don’t'): ['make'],\n",
       " ('phase.', 'Don’t', 'make'): ['excuses'],\n",
       " ('Don’t', 'make', 'excuses'): ['to'],\n",
       " ('make', 'excuses', 'to'): ['use'],\n",
       " ('excuses', 'to', 'use'): ['drugs.'],\n",
       " ('to', 'use', 'drugs.'): ['Don’t'],\n",
       " ('use', 'drugs.', 'Don’t'): ['pass'],\n",
       " ('drugs.', 'Don’t', 'pass'): ['the'],\n",
       " ('Don’t', 'pass', 'the'): ['glass.'],\n",
       " ('pass', 'the', 'glass.'): ['Don’t'],\n",
       " ('the', 'glass.', 'Don’t'): ['smoke'],\n",
       " ('glass.', 'Don’t', 'smoke'): ['Mary'],\n",
       " ('Don’t', 'smoke', 'Mary'): ['Jane,'],\n",
       " ('smoke', 'Mary', 'Jane,'): ['go'],\n",
       " ('Mary', 'Jane,', 'go'): ['date'],\n",
       " ('Jane,', 'go', 'date'): ['her.'],\n",
       " ('go', 'date', 'her.'): ['Don’t'],\n",
       " ('date', 'her.', 'Don’t'): ['snort'],\n",
       " ('her.', 'Don’t', 'snort'): ['your'],\n",
       " ('Don’t', 'snort', 'your'): ['life'],\n",
       " ('snort', 'your', 'life'): ['away.'],\n",
       " ('your', 'life', 'away.'): ['Drug'],\n",
       " ('life', 'away.', 'Drug'): ['free'],\n",
       " ('away.', 'Drug', 'free'): ['is'],\n",
       " ('Drug', 'free', 'is'): ['for'],\n",
       " ('free', 'is', 'for'): ['me.'],\n",
       " ('is', 'for', 'me.'): ['Drugs'],\n",
       " ('for', 'me.', 'Drugs'): ['are'],\n",
       " ('me.', 'Drugs', 'are'): ['so'],\n",
       " ('Drugs', 'are', 'so'): ['yesterday.'],\n",
       " ('are', 'so', 'yesterday.'): ['Drugs'],\n",
       " ('so', 'yesterday.', 'Drugs'): ['will'],\n",
       " ('yesterday.', 'Drugs', 'will'): ['cost'],\n",
       " ('Drugs', 'will', 'cost'): ['you'],\n",
       " ('will', 'cost', 'you'): ['more'],\n",
       " ('cost', 'you', 'more'): ['than'],\n",
       " ('you', 'more', 'than'): ['money.'],\n",
       " ('more', 'than', 'money.'): ['Enjoy'],\n",
       " ('than', 'money.', 'Enjoy'): ['a'],\n",
       " ('money.', 'Enjoy', 'a'): ['song'],\n",
       " ('Enjoy', 'a', 'song'): ['and'],\n",
       " ('a', 'song', 'and'): ['not'],\n",
       " ('song', 'and', 'not'): ['a'],\n",
       " ('and', 'not', 'a'): ['BONG.'],\n",
       " ('not', 'a', 'BONG.'): ['Even'],\n",
       " ('a', 'BONG.', 'Even'): ['Jesus'],\n",
       " ('BONG.', 'Even', 'Jesus'): ['didn’t'],\n",
       " ('Even', 'Jesus', 'didn’t'): ['get'],\n",
       " ('Jesus', 'didn’t', 'get'): ['STONED!'],\n",
       " ('didn’t', 'get', 'STONED!'): ['Family,'],\n",
       " ('get', 'STONED!', 'Family,'): ['friends,'],\n",
       " ('STONED!', 'Family,', 'friends,'): ['and'],\n",
       " ('Family,', 'friends,', 'and'): ['drugs'],\n",
       " ('friends,', 'and', 'drugs'): ['don’t'],\n",
       " ('and', 'drugs', 'don’t'): ['mix.'],\n",
       " ('drugs', 'don’t', 'mix.'): ['Flee'],\n",
       " ('don’t', 'mix.', 'Flee'): ['from'],\n",
       " ('mix.', 'Flee', 'from'): ['the'],\n",
       " ('Flee', 'from', 'the'): ['Big'],\n",
       " ('from', 'the', 'Big'): ['C.'],\n",
       " ('the', 'Big', 'C.'): ['Freebase'],\n",
       " ('Big', 'C.', 'Freebase'): ['is'],\n",
       " ('C.', 'Freebase', 'is'): ['better'],\n",
       " ('Freebase', 'is', 'better'): ['in'],\n",
       " ('is', 'better', 'in'): ['baseball.'],\n",
       " ('better', 'in', 'baseball.'): ['Getting'],\n",
       " ('in', 'baseball.', 'Getting'): ['wasted'],\n",
       " ('baseball.', 'Getting', 'wasted'): ['is'],\n",
       " ('Getting', 'wasted', 'is'): ['a'],\n",
       " ('wasted', 'is', 'a'): ['waste'],\n",
       " ('is', 'a', 'waste'): ['of'],\n",
       " ('a', 'waste', 'of'): ['time.'],\n",
       " ('waste', 'of', 'time.'): ['Go'],\n",
       " ('of', 'time.', 'Go'): ['on'],\n",
       " ('time.', 'Go', 'on'): ['a'],\n",
       " ('Go', 'on', 'a'): ['trip'],\n",
       " ('on', 'a', 'trip'): ['with'],\n",
       " ('a', 'trip', 'with'): ['Carnival'],\n",
       " ('trip', 'with', 'Carnival'): ['Cruise,'],\n",
       " ('with', 'Carnival', 'Cruise,'): ['not'],\n",
       " ('Carnival', 'Cruise,', 'not'): ['drugs.'],\n",
       " ('Cruise,', 'not', 'drugs.'): ['Herb'],\n",
       " ('not', 'drugs.', 'Herb'): ['is'],\n",
       " ('drugs.', 'Herb', 'is'): ['good'],\n",
       " ('Herb', 'is', 'good'): ['for'],\n",
       " ('is', 'good', 'for'): ['food,'],\n",
       " ('good', 'for', 'food,'): ['BAD'],\n",
       " ('for', 'food,', 'BAD'): ['for'],\n",
       " ('food,', 'BAD', 'for'): ['the'],\n",
       " ('BAD', 'for', 'the'): ['brain.'],\n",
       " ('for', 'the', 'brain.'): ['I'],\n",
       " ('the', 'brain.', 'I'): ['got'],\n",
       " ('brain.', 'I', 'got'): ['99'],\n",
       " ('I', 'got', '99'): ['problems,'],\n",
       " ('got', '99', 'problems,'): ['but'],\n",
       " ('99', 'problems,', 'but'): ['drugs'],\n",
       " ('problems,', 'but', 'drugs'): ['ain’t'],\n",
       " ('but', 'drugs', 'ain’t'): ['one.'],\n",
       " ('drugs', 'ain’t', 'one.'): ['I'],\n",
       " ('ain’t', 'one.', 'I'): ['have'],\n",
       " ('one.', 'I', 'have'): ['to'],\n",
       " ('I', 'have', 'to'): ['say'],\n",
       " ('have', 'to', 'say'): ['nope'],\n",
       " ('to', 'say', 'nope'): ['to'],\n",
       " ('say', 'nope', 'to'): ['dope.'],\n",
       " ('nope', 'to', 'dope.'): ['I'],\n",
       " ('to', 'dope.', 'I'): ['simply'],\n",
       " ('dope.', 'I', 'simply'): ['choose'],\n",
       " ('I', 'simply', 'choose'): ['not'],\n",
       " ('simply', 'choose', 'not'): ['to'],\n",
       " ('choose', 'not', 'to'): ['use.'],\n",
       " ('not', 'to', 'use.'): ['I'],\n",
       " ('to', 'use.', 'I'): ['will'],\n",
       " ('use.', 'I', 'will'): ['not'],\n",
       " ('I', 'will', 'not'): ['be'],\n",
       " ('will', 'not', 'be'): ['influenced.'],\n",
       " ('not', 'be', 'influenced.'): ['I’d'],\n",
       " ('be', 'influenced.', 'I’d'): ['rather'],\n",
       " ('influenced.', 'I’d', 'rather'): ['be'],\n",
       " ('I’d', 'rather', 'be'): ['high'],\n",
       " ('rather', 'be', 'high'): ['on'],\n",
       " ('be', 'high', 'on'): ['life'],\n",
       " ('high', 'on', 'life'): ['than'],\n",
       " ('on', 'life', 'than'): ['marijuana.'],\n",
       " ('life', 'than', 'marijuana.'): ['I’m'],\n",
       " ('than', 'marijuana.', 'I’m'): ['glad'],\n",
       " ('marijuana.', 'I’m', 'glad'): ['I’m'],\n",
       " ('I’m', 'glad', 'I’m'): ['not'],\n",
       " ('glad', 'I’m', 'not'): ['“Breaking'],\n",
       " ('I’m', 'not', '“Breaking'): ['Bad”.'],\n",
       " ('not', '“Breaking', 'Bad”.'): ['If'],\n",
       " ('“Breaking', 'Bad”.', 'If'): ['I'],\n",
       " ('Bad”.', 'If', 'I'): ['must'],\n",
       " ('If', 'I', 'must'): ['say'],\n",
       " ('I', 'must', 'say'): ['so,'],\n",
       " ('must', 'say', 'so,'): ['there'],\n",
       " ('say', 'so,', 'there'): ['will'],\n",
       " ('so,', 'there', 'will'): ['be'],\n",
       " ('there', 'will', 'be'): ['no'],\n",
       " ('will', 'be', 'no'): ['yoyo.'],\n",
       " ('be', 'no', 'yoyo.'): ['If'],\n",
       " ('no', 'yoyo.', 'If'): ['you'],\n",
       " ('yoyo.', 'If', 'you'): ['do'],\n",
       " ('If', 'you', 'do'): ['blow,'],\n",
       " ('you', 'do', 'blow,'): ['you'],\n",
       " ('do', 'blow,', 'you'): ['have'],\n",
       " ('blow,', 'you', 'have'): ['to'],\n",
       " ('you', 'have', 'to'): ['go.'],\n",
       " ('have', 'to', 'go.'): ['It’s'],\n",
       " ('to', 'go.', 'It’s'): ['a'],\n",
       " ('go.', 'It’s', 'a'): ['reason'],\n",
       " ('It’s', 'a', 'reason'): ['they'],\n",
       " ('a', 'reason', 'they'): ['call'],\n",
       " ('reason', 'they', 'call'): ['it'],\n",
       " ('they', 'call', 'it'): ['the'],\n",
       " ('call', 'it', 'the'): ['trap'],\n",
       " ('it', 'the', 'trap'): ['house.'],\n",
       " ('the', 'trap', 'house.'): ['Just'],\n",
       " ('trap', 'house.', 'Just'): ['say'],\n",
       " ('house.', 'Just', 'say'): ['no.'],\n",
       " ('Just', 'say', 'no.'): ['Just'],\n",
       " ('say', 'no.', 'Just'): ['be'],\n",
       " ('no.', 'Just', 'be'): ['smart,'],\n",
       " ('Just', 'be', 'smart,'): ['don’t'],\n",
       " ('be', 'smart,', 'don’t'): ['start.'],\n",
       " ('smart,', 'don’t', 'start.'): ['Just'],\n",
       " ('don’t', 'start.', 'Just'): ['listen'],\n",
       " ('start.', 'Just', 'listen'): ['to'],\n",
       " ('Just', 'listen', 'to'): ['“The'],\n",
       " ('listen', 'to', '“The'): ['Chronic”'],\n",
       " ('to', '“The', 'Chronic”'): ['don’t'],\n",
       " ('“The', 'Chronic”', 'don’t'): ['go'],\n",
       " ('Chronic”', 'don’t', 'go'): ['smoke'],\n",
       " ('don’t', 'go', 'smoke'): ['it.'],\n",
       " ('go', 'smoke', 'it.'): ['Just'],\n",
       " ('smoke', 'it.', 'Just'): ['trash'],\n",
       " ('it.', 'Just', 'trash'): ['the'],\n",
       " ('Just', 'trash', 'the'): ['stash.'],\n",
       " ('trash', 'the', 'stash.'): ['Keep'],\n",
       " ('the', 'stash.', 'Keep'): ['BASE'],\n",
       " ('stash.', 'Keep', 'BASE'): ['out'],\n",
       " ('Keep', 'BASE', 'out'): ['of'],\n",
       " ('BASE', 'out', 'of'): ['your'],\n",
       " ('out', 'of', 'your'): ['face.'],\n",
       " ('of', 'your', 'face.'): ['LSD'],\n",
       " ('your', 'face.', 'LSD'): ['is'],\n",
       " ('face.', 'LSD', 'is'): ['not'],\n",
       " ('LSD', 'is', 'not'): ['for'],\n",
       " ('is', 'not', 'for'): ['me.'],\n",
       " ('not', 'for', 'me.'): ['Math'],\n",
       " ('for', 'me.', 'Math'): ['over'],\n",
       " ('me.', 'Math', 'over'): ['Meth.'],\n",
       " ('Math', 'over', 'Meth.'): ['Meth'],\n",
       " ('over', 'Meth.', 'Meth'): ['is'],\n",
       " ('Meth.', 'Meth', 'is'): ['a'],\n",
       " ('Meth', 'is', 'a'): ['REAL'],\n",
       " ('is', 'a', 'REAL'): ['tooth'],\n",
       " ('a', 'REAL', 'tooth'): ['fairy.'],\n",
       " ('REAL', 'tooth', 'fairy.'): ['More'],\n",
       " ('tooth', 'fairy.', 'More'): ['hugs'],\n",
       " ('fairy.', 'More', 'hugs'): ['and'],\n",
       " ('More', 'hugs', 'and'): ['less'],\n",
       " ('hugs', 'and', 'less'): ['drugs.'],\n",
       " ('and', 'less', 'drugs.'): ['Nickel'],\n",
       " ('less', 'drugs.', 'Nickel'): ['bags'],\n",
       " ('drugs.', 'Nickel', 'bags'): ['are'],\n",
       " ('Nickel', 'bags', 'are'): ['for'],\n",
       " ('bags', 'are', 'for'): ['savings.'],\n",
       " ('are', 'for', 'savings.'): ['Nose'],\n",
       " ('for', 'savings.', 'Nose'): ['candy'],\n",
       " ('savings.', 'Nose', 'candy'): ['doesn’t'],\n",
       " ('Nose', 'candy', 'doesn’t'): ['make'],\n",
       " ('candy', 'doesn’t', 'make'): ['since.'],\n",
       " ('doesn’t', 'make', 'since.'): ['Ounces'],\n",
       " ('make', 'since.', 'Ounces'): ['are'],\n",
       " ('since.', 'Ounces', 'are'): ['better'],\n",
       " ('Ounces', 'are', 'better'): ['used'],\n",
       " ('are', 'better', 'used'): ['in'],\n",
       " ('better', 'used', 'in'): ['cooking,'],\n",
       " ('used', 'in', 'cooking,'): ['not'],\n",
       " ('in', 'cooking,', 'not'): ['drugs'],\n",
       " ('cooking,', 'not', 'drugs'): ['Rollin’'],\n",
       " ('not', 'drugs', 'Rollin’'): ['a'],\n",
       " ('drugs', 'Rollin’', 'a'): ['blunt'],\n",
       " ('Rollin’', 'a', 'blunt'): ['is'],\n",
       " ('a', 'blunt', 'is'): ['for'],\n",
       " ('blunt', 'is', 'for'): ['chumps.'],\n",
       " ('is', 'for', 'chumps.'): ['Say'],\n",
       " ('for', 'chumps.', 'Say'): ['no'],\n",
       " ('chumps.', 'Say', 'no'): ['before'],\n",
       " ('Say', 'no', 'before'): ['having'],\n",
       " ('no', 'before', 'having'): ['to'],\n",
       " ('before', 'having', 'to'): ['go'],\n",
       " ('having', 'to', 'go'): ['cold'],\n",
       " ('to', 'go', 'cold'): ['turkey.'],\n",
       " ('go', 'cold', 'turkey.'): ['Smoke'],\n",
       " ('cold', 'turkey.', 'Smoke'): ['fast'],\n",
       " ('turkey.', 'Smoke', 'fast'): ['and'],\n",
       " ('Smoke', 'fast', 'and'): ['die'],\n",
       " ('fast', 'and', 'die'): ['young.'],\n",
       " ('and', 'die', 'young.'): ['Smoking'],\n",
       " ('die', 'young.', 'Smoking'): ['meth'],\n",
       " ('young.', 'Smoking', 'meth'): ['leads'],\n",
       " ('Smoking', 'meth', 'leads'): ['to'],\n",
       " ('meth', 'leads', 'to'): ['death.'],\n",
       " ('leads', 'to', 'death.'): ['Smoking’'],\n",
       " ('to', 'death.', 'Smoking’'): ['a'],\n",
       " ('death.', 'Smoking’', 'a'): ['spliff'],\n",
       " ('Smoking’', 'a', 'spliff'): ['is'],\n",
       " ('a', 'spliff', 'is'): ['no'],\n",
       " ('spliff', 'is', 'no'): ['gift.'],\n",
       " ('is', 'no', 'gift.'): ['The'],\n",
       " ('no', 'gift.', 'The'): ['dope'],\n",
       " ('gift.', 'The', 'dope'): ['trap'],\n",
       " ('The', 'dope', 'trap'): ['is'],\n",
       " ('dope', 'trap', 'is'): ['a'],\n",
       " ('trap', 'is', 'a'): ['trap.'],\n",
       " ('is', 'a', 'trap.'): ['The'],\n",
       " ('a', 'trap.', 'The'): ['green'],\n",
       " ('trap.', 'The', 'green'): ['sticky'],\n",
       " ('The', 'green', 'sticky'): ['is'],\n",
       " ('green', 'sticky', 'is'): ['icky.'],\n",
       " ('sticky', 'is', 'icky.'): ['The'],\n",
       " ('is', 'icky.', 'The'): ['math'],\n",
       " ('icky.', 'The', 'math'): ['lab'],\n",
       " ('The', 'math', 'lab'): ['is'],\n",
       " ('math', 'lab', 'is'): ['cooler'],\n",
       " ('lab', 'is', 'cooler'): ['than'],\n",
       " ('is', 'cooler', 'than'): ['the'],\n",
       " ('cooler', 'than', 'the'): ['meth'],\n",
       " ('than', 'the', 'meth'): ['lab.'],\n",
       " ('the', 'meth', 'lab.'): ['The'],\n",
       " ('meth', 'lab.', 'The'): ['only'],\n",
       " ('lab.', 'The', 'only'): ['coke'],\n",
       " ('The', 'only', 'coke'): ['I'],\n",
       " ('only', 'coke', 'I'): ['do'],\n",
       " ('coke', 'I', 'do'): ['is'],\n",
       " ('I', 'do', 'is'): ['Diet.'],\n",
       " ('do', 'is', 'Diet.'): ['The'],\n",
       " ('is', 'Diet.', 'The'): ['use'],\n",
       " ('Diet.', 'The', 'use'): ['of'],\n",
       " ('The', 'use', 'of'): ['dope'],\n",
       " ('use', 'of', 'dope'): ['brings'],\n",
       " ('of', 'dope', 'brings'): ['no'],\n",
       " ('dope', 'brings', 'no'): ['hope.'],\n",
       " ('brings', 'no', 'hope.'): ['The'],\n",
       " ('no', 'hope.', 'The'): ['ZigZag'],\n",
       " ('hope.', 'The', 'ZigZag'): ['is'],\n",
       " ('The', 'ZigZag', 'is'): ['bad.'],\n",
       " ('ZigZag', 'is', 'bad.'): ['There'],\n",
       " ('is', 'bad.', 'There'): ['is'],\n",
       " ('bad.', 'There', 'is'): ['no'],\n",
       " ('There', 'is', 'no'): ['need'],\n",
       " ('is', 'no', 'need'): ['to'],\n",
       " ('no', 'need', 'to'): ['smoke'],\n",
       " ('need', 'to', 'smoke'): ['on'],\n",
       " ('to', 'smoke', 'on'): ['weed.'],\n",
       " ('smoke', 'on', 'weed.'): ['There’s'],\n",
       " ('on', 'weed.', 'There’s'): ['limited'],\n",
       " ('weed.', 'There’s', 'limited'): ['cope'],\n",
       " ('There’s', 'limited', 'cope'): ['using'],\n",
       " ('limited', 'cope', 'using'): ['dope.'],\n",
       " ('cope', 'using', 'dope.'): ['Think'],\n",
       " ('using', 'dope.', 'Think'): ['twice'],\n",
       " ('dope.', 'Think', 'twice'): ['before'],\n",
       " ('Think', 'twice', 'before'): ['smoking'],\n",
       " ('twice', 'before', 'smoking'): ['ice.'],\n",
       " ('before', 'smoking', 'ice.'): ['Too'],\n",
       " ('smoking', 'ice.', 'Too'): ['much'],\n",
       " ('ice.', 'Too', 'much'): ['abuse'],\n",
       " ('Too', 'much', 'abuse'): ['in'],\n",
       " ('much', 'abuse', 'in'): ['drug'],\n",
       " ('abuse', 'in', 'drug'): ['use.'],\n",
       " ('in', 'drug', 'use.'): ['Up'],\n",
       " ('drug', 'use.', 'Up'): ['the'],\n",
       " ('use.', 'Up', 'the'): ['nose'],\n",
       " ('Up', 'the', 'nose'): ['and'],\n",
       " ('the', 'nose', 'and'): ['down'],\n",
       " ('nose', 'and', 'down'): ['he'],\n",
       " ('and', 'down', 'he'): ['goes.'],\n",
       " ('down', 'he', 'goes.'): ['Weed'],\n",
       " ('he', 'goes.', 'Weed'): ['brownies'],\n",
       " ('goes.', 'Weed', 'brownies'): ['taste'],\n",
       " ('Weed', 'brownies', 'taste'): ['the'],\n",
       " ('brownies', 'taste', 'the'): ['same'],\n",
       " ('taste', 'the', 'same'): ['as'],\n",
       " ('the', 'same', 'as'): ['regular'],\n",
       " ('same', 'as', 'regular'): ['brownies.'],\n",
       " ('as', 'regular', 'brownies.'): ['Weed'],\n",
       " ('regular', 'brownies.', 'Weed'): ['is'],\n",
       " ('brownies.', 'Weed', 'is'): ['for'],\n",
       " ('Weed', 'is', 'for'): ['wimps.'],\n",
       " ('is', 'for', 'wimps.'): ['Where'],\n",
       " ('for', 'wimps.', 'Where'): ['diamonds,'],\n",
       " ('wimps.', 'Where', 'diamonds,'): ['don’t'],\n",
       " ('Where', 'diamonds,', 'don’t'): ['smoke'],\n",
       " ('diamonds,', 'don’t', 'smoke'): ['them.'],\n",
       " ('don’t', 'smoke', 'them.'): ['You'],\n",
       " ('smoke', 'them.', 'You'): ['are'],\n",
       " ('them.', 'You', 'are'): ['bound'],\n",
       " ('You', 'are', 'bound'): ['to'],\n",
       " ('are', 'bound', 'to'): ['fall'],\n",
       " ('bound', 'to', 'fall'): ['if'],\n",
       " ('to', 'fall', 'if'): ['you'],\n",
       " ('fall', 'if', 'you'): ['smoke'],\n",
       " ('if', 'you', 'smoke'): ['a'],\n",
       " ('you', 'smoke', 'a'): ['meth'],\n",
       " ('smoke', 'a', 'meth'): ['seed'],\n",
       " ('a', 'meth', 'seed'): ['ball.'],\n",
       " ('meth', 'seed', 'ball.'): ['You’ll'],\n",
       " ('seed', 'ball.', 'You’ll'): ['never'],\n",
       " ('ball.', 'You’ll', 'never'): ['be'],\n",
       " ('You’ll', 'never', 'be'): ['as'],\n",
       " ('never', 'be', 'as'): ['high'],\n",
       " ('be', 'as', 'high'): ['as'],\n",
       " ('as', 'high', 'as'): ['GOD.'],\n",
       " ('high', 'as', 'GOD.'): [None]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antidrug_markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"Please don't confront me with my failures I had not forgotten them$\"\n",
    "model = {}\n",
    "for i in range(len(src)-2):\n",
    "    ngram = tuple(src[i:i+2])\n",
    "    next_item = src[i+2]\n",
    "    if ngram not in model:\n",
    "        model[ngram]= []\n",
    "    model[ngram].append(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('P', 'l'): ['e'],\n",
       " ('l', 'e'): ['a'],\n",
       " ('e', 'a'): ['s'],\n",
       " ('a', 's'): ['e'],\n",
       " ('s', 'e'): [' '],\n",
       " ('e', ' '): ['d', 'w'],\n",
       " (' ', 'd'): ['o'],\n",
       " ('d', 'o'): ['n'],\n",
       " ('o', 'n'): [\"'\", 'f', 't'],\n",
       " ('n', \"'\"): ['t'],\n",
       " (\"'\", 't'): [' '],\n",
       " ('t', ' '): ['c', 'm', 'f'],\n",
       " (' ', 'c'): ['o'],\n",
       " ('c', 'o'): ['n'],\n",
       " ('n', 'f'): ['r'],\n",
       " ('f', 'r'): ['o'],\n",
       " ('r', 'o'): ['n'],\n",
       " ('n', 't'): [' '],\n",
       " (' ', 'm'): ['e', 'y'],\n",
       " ('m', 'e'): [' '],\n",
       " (' ', 'w'): ['i'],\n",
       " ('w', 'i'): ['t'],\n",
       " ('i', 't'): ['h'],\n",
       " ('t', 'h'): [' ', 'e'],\n",
       " ('h', ' '): ['m'],\n",
       " ('m', 'y'): [' '],\n",
       " ('y', ' '): ['f'],\n",
       " (' ', 'f'): ['a', 'o'],\n",
       " ('f', 'a'): ['i'],\n",
       " ('a', 'i'): ['l'],\n",
       " ('i', 'l'): ['u'],\n",
       " ('l', 'u'): ['r'],\n",
       " ('u', 'r'): ['e'],\n",
       " ('r', 'e'): ['s'],\n",
       " ('e', 's'): [' '],\n",
       " ('s', ' '): ['I'],\n",
       " (' ', 'I'): [' '],\n",
       " ('I', ' '): ['h'],\n",
       " (' ', 'h'): ['a'],\n",
       " ('h', 'a'): ['d'],\n",
       " ('a', 'd'): [' '],\n",
       " ('d', ' '): ['n'],\n",
       " (' ', 'n'): ['o'],\n",
       " ('n', 'o'): ['t'],\n",
       " ('o', 't'): [' ', 't'],\n",
       " ('f', 'o'): ['r'],\n",
       " ('o', 'r'): ['g'],\n",
       " ('r', 'g'): ['o'],\n",
       " ('g', 'o'): ['t'],\n",
       " ('t', 't'): ['e'],\n",
       " ('t', 'e'): ['n'],\n",
       " ('e', 'n'): [' '],\n",
       " ('n', ' '): ['t'],\n",
       " (' ', 't'): ['h'],\n",
       " ('h', 'e'): ['m'],\n",
       " ('e', 'm'): ['$']}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shmarkov import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterdays_str = open(\"betterdays.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When your love has moved away\\nYou must face yourself and you must say\\nI remember better days\\nDon't you cry 'cause she is gone\\nShe is only moving on\\nChasing mirrors through a haze\\n\\nNow that you know it's nowhere\\nWhat's to stop you coming home\\nAll you got to do is go there\\nThen you'll really realize what's going down\\n\\nYou went to a strange land searching\\nFor a truth you felt was wrong\\nThat's when the heartaches started\\nThough you're where you want to be, you're not where you belong\\n\\nWhen your love has moved away\\nYou must face yourself and you must say\\nI remember better days\\nDon't you cry 'cause she is gone\\nShe is only moving on\\nChasing mirrors through a haze\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterdays_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markov_model(4, betterdays_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r days\n",
      "Don't your love has moving down\n",
      "\n",
      "You must say\n",
      "I remember better days\n",
      "Don't your love has moved aw\n"
     ]
    }
   ],
   "source": [
    "print(''.join(gen_from_model(4, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesedays_str = open(\"thesedays.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've been out walking\\nI don't do too much talking these days\\nThese days\\nThese days I seem to think a lot\\nAbout the things that I forgot to do\\nAnd all the times I had the chance to\\n\\nI stopped my rambling\\nI don't do too much gambling these days\\nThese days\\nThese days I seem to think about how all the changes came about my way\\nAnd I wonder if I'll see another highway\\n\\nI had a lover\\nI don't think I'd risk another these days\\nThese days\\nAnd if I seem to be afraid\\nTo live the life that I have made in song\\nIt's just that I've been losing so long\\n\\nI stopped my dreaming\\nI won't do too much scheming these days\\nThese days\\nThese days I sit on cornerstones\\nAnd count the time in quarter tones to 10\\nPlease don't confront me with my failures\\nI had not forgotten them\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesedays_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markov_model(5, thesedays_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ut the things that I have made in quarter tones\n",
      "And count the time in quarter tones to 10\n",
      "Please don't th\n"
     ]
    }
   ],
   "source": [
    "print(''.join(gen_from_model(5, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = open(\"betterdays.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_b = open(\"thesedays.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a, state_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_a.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Though you're where you belong.\""
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_a.make_short_sentence(40, tries=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n",
      "Though you're where you belong.\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(generator_a.make_short_sentence(60, tries=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_b = markovify.Text(text_b, state_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't do too much scheming these days.\""
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_b.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These days I seem to be afraid.'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_b.make_short_sentence(40, tries=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's just that I've been out walking.\n",
      "These days I seem to be afraid.\n",
      "I won't do too much talking these days.\n",
      "I won't do too much gambling these days.\n",
      "These days I seem to be afraid.\n",
      "These days I seem to be afraid.\n",
      "I don't do too much scheming these days.\n",
      "It's just that I've been out walking.\n",
      "About the things that I have made in song.\n",
      "I won't do too much talking these days.\n",
      "I won't do too much gambling these days.\n",
      "I don't do too much scheming these days.\n",
      "These days I seem to be afraid.\n",
      "To live the life that I forgot to do.\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(generator_b.make_short_sentence(60, tries=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = markovify.combine([generator_a, generator_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Though you're where you want to be, you're not where you want to be, you're not where you belong.\""
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_c = open(\"roxanegaymanifesto.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_c = markovify.Text(text_c, state_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We forget the difference between feminism and shun the feminist label but say they support all the answers.'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_c.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_d = open(\"afewwordsaboutbreasts.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_d = markovify.Text(text_d, state_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had no restraints about needling me—why did they say nothing as they watched my chest that occasionally crashed into a wall and was poked inward and had many of the time.”'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_d.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = markovify.combine([generator_c, generator_d], [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_generate_from_sequences(n, sequences, count, max_gen=100):\n",
    "    starts = [item[:n] for item in sequences if len(item) >= n]\n",
    "    model = markov_model_from_sequences(n, sequences)\n",
    "    return [gen_from_model(n, model, random.choice(starts), max_gen)\n",
    "           for i in range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = markovify.combine([generator_c, generator_d], [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I believe women should be paid as much as men for the same people who fear feminism most, the same feminism.'"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterdays_lines = [line.strip() for line in open(\"betterdays.txt\").readlines()]\n",
    "for item in markov_generate_from_sequences(5, betterdays_lines, 20):\n",
    "    print(''.join(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesedays_lines = [line.strip() for line in open(\"thesedays.txt\").readlines()]\n",
    "for item in markov_generate_from_sequences(5, thesedays_lines, 20):\n",
    "    print(''.join(item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
